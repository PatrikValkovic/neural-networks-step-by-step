{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing, sklearn.datasets, sklearn.model_selection\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we talked just about the models. I shown you some plots and I tried to explain some phenomena on them. The results usually seems to be right, however I never talked about the evaluation of the models. How to recognize, if the models is learning and performing well. I would like to correct this in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and metric\n",
    "\n",
    "First of all, let's talk about losses and metrices. Loss is the function, that the learning algorithm directly optimize. So far we used mean squared error loss (MSE), that is in the form $\\frac{1}{n} \\sum_{i=0}^{n} \\left( t_i - f(\\pmb{x_i}) \\right)^2$. As $n$ is the constant (we have finite set of examples), optimizing it is the same as optimizing $\\frac{1}{2} \\sum_{i=0}^{n} \\left( t_i - f(\\pmb{x_i}) \\right)^2$. We computed gradient of this value and optimize it directly. Before moving on, let's remind us the `Neuron` class from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, epochs=100, random_state=None, learning_rate=0.001, batch_size=16):\n",
    "        # use epochs instead of iterations\n",
    "        self.epochs = epochs\n",
    "        self.converged = False\n",
    "        self._rand = np.random.RandomState(random_state)\n",
    "        self._weights = None\n",
    "        self._learning_rate = learning_rate\n",
    "        self._batch_size = batch_size\n",
    "        pass\n",
    "    \n",
    "    def _activation(self, vals):  # activation function\n",
    "        return 1 / (1 + np.exp(-vals))\n",
    "    \n",
    "    def fit(self, X, y, Xtest=None, ytest=None):\n",
    "        # Initialize the weights\n",
    "        self.converged = False\n",
    "        self._weights = self._rand.uniform(-2, 2, X.shape[1])\n",
    "        n_data = len(X)\n",
    "        # store gradients and losses\n",
    "        losses = np.zeros((self.epochs,))\n",
    "        gradients = np.zeros((self.epochs, int(np.ceil(X.shape[0] / self._batch_size))))\n",
    "        accuracies = np.zeros((self.epochs,))\n",
    "        # Learning\n",
    "        for epoch in range(self.epochs):\n",
    "            # shuffle the data\n",
    "            permutation = self._rand.permutation(n_data)\n",
    "            # for each batch\n",
    "            for batch_start in range(0, n_data, self._batch_size):\n",
    "                # get batch\n",
    "                batch_data = X[permutation[batch_start:batch_start+self._batch_size]]\n",
    "                batch_target = y[permutation[batch_start:batch_start+self._batch_size]]\n",
    "                # predict the data\n",
    "                prediction = self.predict(batch_data)\n",
    "                # table of gradient for each weights and gradient in the shape (samples,weights)\n",
    "                gradient = np.reshape(-(batch_target - prediction) * (prediction * (1 - prediction)), newshape=(-1,1)) * batch_data\n",
    "                # mean gradient over the samples\n",
    "                op_gradient = self._learning_rate * np.sum(gradient, axis=0)\n",
    "                # store losses and gradients\n",
    "                losses[epoch] += np.sum((batch_target - prediction) ** 2, axis=0)\n",
    "                gradients[epoch, int(batch_start / self._batch_size)] = op_gradient @ op_gradient\n",
    "                # update the weights\n",
    "                self._weights = self._weights - op_gradient\n",
    "                \n",
    "            # compute accuracy on the test set\n",
    "            if Xtest is not None and ytest is not None:\n",
    "                test_prediction = np.where(self.predict(Xtest) < 0.5, 0, 1) \n",
    "                accuracies[epoch] = np.sum(test_prediction == ytest) / len(ytest)\n",
    "                \n",
    "        return losses / X.shape[0], gradients, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "            return self._activation(X @ self._weights)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method return losses, gradients and accuracies over the epochs. As we can see from the code, we compute gradient for each example and update weights. In the end, the method returns `losses / X.shape[0]`, that is the mean loss over the whole dataset.\n",
    "\n",
    "There is one important aspect to understand here - loss is independent between the examples. As the loss is the value, that is directly optimized, it should always decrease. If we had only single example and use this algorithm, it needs to decrease every step (unless the loss is zero). However, as optimizing loss for one example can increase the loss of other example (and loss is  independent for each exaple), the loss of the whole dataset (mean of losses for each example) can increase.\n",
    "\n",
    "How can we make sure this doesn't happen? We may optimize the dataset loss instead of loss per each example - that is exactly what the batch gradient descent do! Although it still computes the loss for each example, there is only one update, that takes into account all the examples. As such, it optimize the dataset loss and the loss never increase (as you can see in the end of the previous notebook). As you may guess, using minibatch gradient descent we try to use this property using less examples.\n",
    "\n",
    "Through the following notebooks we are going to see a few more losses. They usage depends on the task we are doing - for regresion tasks (we try to predict continuous value like age, weights and so on) we use MSE loss. For classification tasks (we try to predict in which class the example is from) we usually use cross-entropy loss function. Note that we were doing only classification so far, and we still used MSE loss. It was good enought for out purposes so far, but we will talk about cross-entropy soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's talk about metrices. Metric, unlike loss, is not directly optimized during training. Decrease in loss should result in better metric. In the last notebook we have seen accuracy - how many examples are classified correctly. I will use accuracy to demonstrate some interesting aspects.\n",
    "\n",
    "Unlike loss, the metric doesn't need to decrease. Especially for accuracy, the better the model, the higher accuracy you will get. The accuracy should take more sense for you are as a programmer (or as data scientist) rather than for a computer. It may decrease, increase, oscilate around some value and so on. Its up to you to interpret it.\n",
    "\n",
    "Secondly, metric is not independent between examples. Compute accuracy for subset of dataset doesn't make much sense. Although we may think about accuracy as mean of values 0 (when the prediction was wrong) and 1 (when the prediction was correct), it doesnt need to be this way. You may see some matrices on the [tensorflow documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/metrics). There are metrices like false positive, hinge, recall and so on. The important fact is that metric, unlike loss, is some value that is aggregated over the whole dataset, rather than computed independently for each example.\n",
    "\n",
    "And lastly, metric is optimized indirectly. In our example, we predicted class $0$ if the value was $<0.5$ and $1$ otherwise. Take for example, that our model returned $0.7$ and we corrected it, so it returns $0.6$ (it is the class $0$). In this case, the loss decreased, but the accuracy remained same, as the prediction is still class $1$, although it has lower probability. This should demonstrace you, that not only loss, but why the metrices are important as well. Usually, when you have task in hand, you are not interested in the model loss, but rather in some of it's metric. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing, sklearn.datasets, sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the previous notebook, I briefly showed one-vs-rest classification technique and normalization of the distribution. We may put this normalization after the sigmoid activation functions - that exactly, what softmax is suppose to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "Formally, we may define the softmax as follow:\n",
    "\n",
    "$$\n",
    "softmax(\\pmb{x})_i = \\frac{e^{x_i}}{\\sum_k e^{x_k}}\n",
    "$$\n",
    "\n",
    "In other word, we apply exponential to each input of the softmax and then normalize the probability distribution.\n",
    "\n",
    "Note that softmax is just a generalization of the sigmoid activation.\n",
    "\n",
    "$$\n",
    "softmax(x, 0) = \\frac{e^x}{e^x + e^0} = \\frac{e^x}{e^x + 1} \\cdot \\frac{e^{-x}}{e^{-x}} = \\frac{e^{x-x}}{e^{x-x} + e^{-x}} = \\frac{e^0}{e^0 + e^{-x}} = \\frac{1}{1+e^{-x}} = \\sigma(x)\n",
    "$$\n",
    "\n",
    "Now, when we have the formula, it is easy to implement the sigmoid in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why to bother with sigmoid and not just normalize the outputs of sigmoids? It turns out, we may get better numerical properties if we join sigmoids and normalization into one \"layer\". Thanks to the following equation.\n",
    "\n",
    "$$\n",
    "softmax(\\pmb{x}+c)_i=\\frac{e^{x_i+c}}{\\sum_k e^{x_k+c}} = \\frac{e^c}{e^c} \\cdot \\frac{e^{x_i}}{\\sum_k e^{x_k}} = softmax(\\pmb{x})_i\n",
    "$$\n",
    "\n",
    "The danger in the softmax are the exponents, when the divisor can become too big for float values and as a result become infinity. However, we may set $c=max(\\pmb{x})$ and as all the scalars are negative or zero, there would be no overflow. Cases, where the divisor become zero are much more unlikely and even if some scalars are so small, that they are round to zero because of the float precision, it doesn't matter for use to distinguish between their actual value and zero. \n",
    "\n",
    "This way, we have our modified softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Before we move on, we need to know how to compute gradient. That is not that easy as with softmax, as we have multiple inputs and multiple outputs. You may try it by hand, if you wish.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial softmax(\\pmb{x})_i}{\\partial x_j} = \\frac{\\partial \\frac{e^{x_i}}{\\sum_k e^{x_k}}}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "There are three indices! Moreover, this is derivative only in respect to one input variable. We need to compute gradient of every output in respect to every input. I don't want to dig too much into the math, you can see [[1]](#Bibliography) if you wanna know more. In the end, the gradient pops out, so we may implement it.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial softmax(\\pmb{x})_i}{\\partial x_j} = \n",
    "\\begin{cases}\n",
    "    softmax(\\pmb{x})_j - softmax(\\pmb{x})_j \\cdot softmax(\\pmb{x})_i & \\text{if } i = j \\\\\n",
    "    0 - softmax(\\pmb{x})_j \\cdot softmax(\\pmb{x})_i & \\text{if } i \\neq j \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "You may notice something strange - the gradient is table. Remember, we compute gradient of each output in respect to each input. That need's to be a table. But we want only gradients in respect to the inputs - we need to sum the gradients over the outputs, the same way, as we sum gradients of multiple examples. I won't show the implementation just yet, as I believe it would be more helpfull to view the code and the explanation in bigger picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned **layer** a while ago. The fact is, the softmax is really a layer, how we understand them in the context of neural networks. We will implement our first simple neural networks in the very next notebook. It tooks us eight notebooks, but we finally have enough knowledge to do that. However, we need some refactoring of the code before we can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring\n",
    "\n",
    "Let's start with the **layer** implementation itself. Maybe I should tell first, what the layer is. During the neural network training (we may thing about the logistic regression as about a simple neural network) we are exploiting the chain rule. You may remember from the school the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(g(h(x)))}{\\partial x} = \\frac{\\partial f(g(h(x)))}{\\partial g(h(x))} \\cdot \\frac{\\partial g(h(x))}{\\partial h(x)} \\cdot \\frac{\\partial h(x)}{\\partial x}\n",
    "$$\n",
    "\n",
    "This is not different from our model, remember that loss of logistic regression was written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\sigma(\\pmb{x}\\pmb{w})\n",
    "$$\n",
    "\n",
    "And we need to update the weights.\n",
    "\n",
    "$$\n",
    "\\frac{\\mathcal{L}}{\\partial \\pmb{w}} = \\frac{\\partial \\mathcal{L}}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial \\pmb{x}\\pmb{w}} \\cdot \\frac{\\partial \\pmb{x}\\pmb{w}}{\\partial \\pmb{w}}\n",
    "$$\n",
    "\n",
    "Each part of the gradient will be one layer. We will use the same layers for neural nerworks later on. We already implemented some - for example the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    def __call__(self, target, predicted):\n",
    "        indices = np.arange(len(target))\n",
    "        return -np.log(np.maximum(predicted[indices,target], 1e-15))\n",
    "    \n",
    "    def gradient(self, target, predicted):\n",
    "        grad = np.zeros((len(target), 10))\n",
    "        indices = np.arange(len(target))\n",
    "        grad[indices,target] = -1 / predicted[indices,target]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that the layer accepts output of the previous layer (in this case in shape `(batchsize, classes)`) and it returns it's gradient in respect to it's input - again in the shape `(batchsize,classes`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use the same approach - let's now implement the sigmoid layer. It takes inputs and apply sigmoid activation function on each of it. It is simple to implement it right away, but there is one problem. If you carefully look at the chain rule, the gradients are multiplied by the gradients of the following layer. In other word, by computing gradient of the sigmoid, we need to compute this:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathcal{L}}{\\partial \\pmb{w}} = \\frac{\\partial \\mathcal{L}}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial \\pmb{x}\\pmb{w}}\n",
    "$$\n",
    "\n",
    "As a result, the gradient function doesn't accept only layer's input, but gradient from the following layer as well. Now we may implement the sigmoid activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __call__(self, inputs):\n",
    "        return 1 / (1 + np.exp(-inputs))\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        outputs = self(inputs)\n",
    "        my_gradient = outputs * (1 - outputs)\n",
    "        return my_gradient * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the softmax layer. This case is a bit complicated, as the gradient is not a vector, but a table. In general, most layers that have multiple inputs and multiple outputs will generate gradient table (note that for each example), as it needs to compute gradient of each output in respect to each input. The sigmoid activation layer, and activation layers in general, are exceptions. Each output corresponds to one input and as such the inputs doesn't interfere. You may look at it from the other side, the gradient of output $i$ in respect to input $j$ is zero if $i \\neq j$. That results in table full of zeros expect diagonals.\n",
    "\n",
    "But let's return back to the softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs - np.max(inputs)\n",
    "        return np.exp(inputs) / np.sum(np.exp(inputs), axis=-1)[:,np.newaxis]\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        outputs = self(inputs)\n",
    "        diag = np.zeros((outputs.shape[0], outputs.shape[1], outputs.shape[1]))\n",
    "        my_gradient = np.diag(outputs) - outputs.T @ outputs\n",
    "        return np.sum(gradients[np.newaxis,:] * my_gradient, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (10,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-325-7a6f35b77a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0ml_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0ms_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-162-b4f04008542a>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, inputs, gradients)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdiag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mmy_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmy_gradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (10,10) "
     ]
    }
   ],
   "source": [
    "target = np.random.randint(0,10,size=(3,), dtype=int)\n",
    "vals = np.random.uniform(size=(3,10))\n",
    "soft = SoftmaxLayer()\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "predicted = soft(vals)\n",
    "l = loss(target, predicted)\n",
    "l_grad = loss.gradient(target, predicted)\n",
    "s_grad = soft.gradient(vals, l_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "target_tf = tf.Variable(target)\n",
    "vals_tf = tf.Variable(vals)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predicted_tf = tf.nn.softmax(vals_tf)\n",
    "    l_tf = tf.keras.losses.sparse_categorical_crossentropy(target_tf, predicted_tf)\n",
    "\n",
    "l_grad_tf, s_grad_tf = tape.gradient(l_tf, [predicted_tf, vals_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "[[0.16005908 0.06725888 0.10924701 0.11384503 0.08331395 0.06459966\n",
      "  0.1163387  0.06519166 0.06509037 0.15505567]\n",
      " [0.08421609 0.07562292 0.08035415 0.10763828 0.13765494 0.09973486\n",
      "  0.08157759 0.07799766 0.17106817 0.08413533]\n",
      " [0.14815491 0.09326063 0.08769906 0.12453189 0.05808831 0.07857035\n",
      "  0.05885886 0.08420299 0.13066567 0.13596733]]\n",
      "loss\n",
      "[2.73197873 2.47436928 1.99534064]\n",
      "loss grad\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.         -15.36325671   0.        ]\n",
      " [-11.87421547   0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.          -7.35470788]]\n",
      "softmax grad\n",
      "[[5.79791929 5.40976589 5.79812171 5.76095188 5.76404722 5.58312189\n",
      "  5.29561435 5.88078342 5.96542029 5.89731699]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted)\n",
    "print(\"loss\")\n",
    "print(l)\n",
    "print(\"loss grad\")\n",
    "print(l_grad)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tf.Tensor(\n",
      "[[0.16005908 0.06725888 0.10924701 0.11384503 0.08331395 0.06459966\n",
      "  0.1163387  0.06519166 0.06509037 0.15505567]\n",
      " [0.08421609 0.07562292 0.08035415 0.10763828 0.13765494 0.09973486\n",
      "  0.08157759 0.07799766 0.17106817 0.08413533]\n",
      " [0.14815491 0.09326063 0.08769906 0.12453189 0.05808831 0.07857035\n",
      "  0.05885886 0.08420299 0.13066567 0.13596733]], shape=(3, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor([2.73197873 2.47436928 1.99534064], shape=(3,), dtype=float64)\n",
      "loss grad\n",
      "tf.Tensor(\n",
      "[[  1.           1.           1.           1.           1.\n",
      "    1.           1.           1.         -14.36325671   1.        ]\n",
      " [-10.87421547   1.           1.           1.           1.\n",
      "    1.           1.           1.           1.           1.        ]\n",
      " [  1.           1.           1.           1.           1.\n",
      "    1.           1.           1.           1.          -6.35470788]], shape=(3, 10), dtype=float64)\n",
      "softmax grad\n",
      "tf.Tensor(\n",
      "[[ 0.16005908  0.06725888  0.10924701  0.11384503  0.08331395  0.06459966\n",
      "   0.1163387   0.06519166 -0.93490963  0.15505567]\n",
      " [-0.91578391  0.07562292  0.08035415  0.10763828  0.13765494  0.09973486\n",
      "   0.08157759  0.07799766  0.17106817  0.08413533]\n",
      " [ 0.14815491  0.09326063  0.08769906  0.12453189  0.05808831  0.07857035\n",
      "   0.05885886  0.08420299  0.13066567 -0.86403267]], shape=(3, 10), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_tf)\n",
    "print(\"loss\")\n",
    "print(l_tf)\n",
    "print(\"loss grad\")\n",
    "print(l_grad_tf)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "target_t = torch.tensor(target, dtype=torch.long)\n",
    "vals_t = torch.tensor(vals, requires_grad=True)\n",
    "\n",
    "predicted_t = torch.nn.functional.softmax(vals_t, dim=1)\n",
    "predicted_t.retain_grad()\n",
    "l_t = torch.nn.functional.nll_loss(torch.log(predicted_t), target_t, reduction='none')\n",
    "l_t.backward(torch.ones(l_t.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tensor([[0.1601, 0.0673, 0.1092, 0.1138, 0.0833, 0.0646, 0.1163, 0.0652, 0.0651,\n",
      "         0.1551],\n",
      "        [0.0842, 0.0756, 0.0804, 0.1076, 0.1377, 0.0997, 0.0816, 0.0780, 0.1711,\n",
      "         0.0841],\n",
      "        [0.1482, 0.0933, 0.0877, 0.1245, 0.0581, 0.0786, 0.0589, 0.0842, 0.1307,\n",
      "         0.1360]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "loss\n",
      "tensor([2.7320, 2.4744, 1.9953], dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward>)\n",
      "loss grad\n",
      "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000, -15.3633,   0.0000],\n",
      "        [-11.8742,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  -7.3547]], dtype=torch.float64)\n",
      "softmax grad\n",
      "tensor([[ 0.1601,  0.0673,  0.1092,  0.1138,  0.0833,  0.0646,  0.1163,  0.0652,\n",
      "         -0.9349,  0.1551],\n",
      "        [-0.9158,  0.0756,  0.0804,  0.1076,  0.1377,  0.0997,  0.0816,  0.0780,\n",
      "          0.1711,  0.0841],\n",
      "        [ 0.1482,  0.0933,  0.0877,  0.1245,  0.0581,  0.0786,  0.0589,  0.0842,\n",
      "          0.1307, -0.8640]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_t)\n",
    "print(\"loss\")\n",
    "print(l_t)\n",
    "print(\"loss grad\")\n",
    "print(predicted_t.grad)\n",
    "print(\"softmax grad\")\n",
    "print(vals_t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "- \\[1\\] The Softmax Function Derivative, Stephen Oman, 17th June 2019, [online](https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

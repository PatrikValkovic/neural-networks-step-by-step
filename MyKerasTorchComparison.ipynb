{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing, sklearn.datasets, sklearn.model_selection\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    def __call__(self, target, predicted):\n",
    "        indices = np.arange(len(target))\n",
    "        return -np.log(np.maximum(predicted[indices,target], 1e-15))\n",
    "    \n",
    "    def gradient(self, target, predicted):\n",
    "        grad = np.zeros((len(target), 10))\n",
    "        indices = np.arange(len(target))\n",
    "        grad[indices,target] = -1 / predicted[indices,target]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs - np.max(inputs)\n",
    "        return np.exp(inputs) / np.sum(np.exp(inputs), axis=-1)[:,np.newaxis]\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        outputs = self(inputs)  # examples, classes\n",
    "        examples, classes = outputs.shape\n",
    "        diag = np.zeros((examples, classes, classes))  # examples, classes, classes\n",
    "        diag[:, np.arange(classes), np.arange(classes)] = outputs # set the diagonal of each example\n",
    "        my_gradient = diag - outputs[:,:,np.newaxis] * outputs[:,np.newaxis,:]  # examples, classes, classes\n",
    "        return np.sum(gradients[:,np.newaxis,:] * my_gradient, axis=2) # examples, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self,inputs, outputs, random_seed=None):\n",
    "        self._random_state = np.random.RandomState(random_seed)\n",
    "        self._W = self._random_state.uniform(-2,2,size=(inputs, outputs))\n",
    "        self._b = self._random_state.uniform(-2,2,size=(outputs,))\n",
    "        self.params = [self._W, self._b]\n",
    "        self.grads = [np.zeros_like(self._W), np.zeros_like(self._b)]\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        return inputs @ self._W + self._b[np.newaxis,:]\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        # gradient in respect to W\n",
    "        w_grad = inputs[:,:,np.newaxis] * gradients[:,np.newaxis,:]  # examples, inputs, outputs\n",
    "        np.add(self.grads[0], np.sum(w_grad, axis=0), out=self.grads[0])  # inputs, outputs\n",
    "        # gradient in respect to b\n",
    "        b_grad = gradients  # examples, outputs\n",
    "        np.add(self.grads[1], np.sum(gradients, axis=0), out=self.grads[1])  # outputs\n",
    "        # gradient in respect to inputs\n",
    "        in_grad = self._W[np.newaxis,:,:] * gradients[:,np.newaxis,:] + np.sign(self._b)[np.newaxis, np.newaxis, :]  # examples, inputs, outputs\n",
    "        return np.sum(in_grad, axis=2) # examples, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.random.randint(0,10,size=(3,), dtype=int)\n",
    "vals = np.random.uniform(size=(3,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = DenseLayer(7,10,42)\n",
    "soft = SoftmaxLayer()\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "first = dense(vals)\n",
    "predicted = soft(first)\n",
    "l = loss(target, predicted)\n",
    "l_grad = loss.gradient(target, predicted)\n",
    "s_grad = soft.gradient(first, l_grad)\n",
    "f_grad = dense.gradient(vals, s_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w_tf = tf.Variable(dense._W)\n",
    "b_tf = tf.Variable(dense._b)\n",
    "target_tf = tf.Variable(target)\n",
    "vals_tf = tf.Variable(vals)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    first_tf = vals_tf @ w_tf + b_tf\n",
    "    predicted_tf = tf.nn.softmax(first_tf)\n",
    "    l_tf = tf.keras.losses.sparse_categorical_crossentropy(target_tf, predicted_tf)\n",
    "\n",
    "l_grad_tf, s_grad_tf, w_tf_grad, b_tf_grad, f_tf_grad = tape.gradient(l_tf, [predicted_tf, first_tf, w_tf, b_tf, vals_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w_t = torch.tensor(dense._W, requires_grad=True)\n",
    "b_t = torch.tensor(dense._b, requires_grad=True)\n",
    "target_t = torch.tensor(target, dtype=torch.long)\n",
    "vals_t = torch.tensor(vals, requires_grad=True)\n",
    "\n",
    "first_t = vals_t @ w_t + b_t\n",
    "first_t.retain_grad()\n",
    "predicted_t = torch.nn.functional.softmax(first_t, dim=1)\n",
    "predicted_t.retain_grad()\n",
    "l_t = torch.nn.functional.nll_loss(torch.log(predicted_t), target_t, reduction='none')\n",
    "l_t.backward(torch.ones(l_t.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "[[1.12266094e-02 3.40723142e-02 1.47358772e-03 8.03037682e-01\n",
      "  1.13033016e-02 1.04788833e-01 1.37781349e-03 6.22199035e-03\n",
      "  2.58615332e-02 6.36335544e-04]\n",
      " [6.35036520e-02 4.09659816e-03 4.64408156e-03 4.40106757e-01\n",
      "  3.58909784e-02 4.03573917e-01 2.27405181e-03 2.21779879e-02\n",
      "  1.45106324e-02 9.22134397e-03]\n",
      " [6.83276413e-02 2.99907419e-03 2.12594883e-03 4.72575321e-01\n",
      "  6.79847118e-02 3.54894315e-01 7.18306216e-03 5.94326080e-03\n",
      "  1.21204946e-02 5.84617006e-03]]\n",
      "loss\n",
      "[6.52005522 3.32726931 4.41285749]\n",
      "loss grad\n",
      "[[   0.            0.         -678.61586049    0.            0.\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.          -27.86215492\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.            0.            0.          -82.50488373    0.        ]]\n",
      "softmax grad\n",
      "[[ 1.12266094e-02  3.40723142e-02 -9.98526412e-01  8.03037682e-01\n",
      "   1.13033016e-02  1.04788833e-01  1.37781349e-03  6.22199035e-03\n",
      "   2.58615332e-02  6.36335544e-04]\n",
      " [ 6.35036520e-02  4.09659816e-03  4.64408156e-03  4.40106757e-01\n",
      "  -9.64109022e-01  4.03573917e-01  2.27405181e-03  2.21779879e-02\n",
      "   1.45106324e-02  9.22134397e-03]\n",
      " [ 6.83276413e-02  2.99907419e-03  2.12594883e-03  4.72575321e-01\n",
      "   6.79847118e-02  3.54894315e-01  7.18306216e-03  5.94326080e-03\n",
      "  -9.87879505e-01  5.84617006e-03]]\n",
      "W grad\n",
      "[[ 0.05638084  0.03019958 -0.80033036  0.97396841 -0.50256216  0.37033739\n",
      "   0.00369181  0.01818242 -0.15649144  0.0066235 ]\n",
      " [ 0.02387334  0.02321265 -0.65145359  0.63923074 -0.15216132  0.16699908\n",
      "   0.00188468  0.00835719 -0.06242252  0.00247975]\n",
      " [ 0.08376848  0.02364808 -0.55887248  0.98933851 -0.72128376  0.51907598\n",
      "   0.00546968  0.02329163 -0.37438767  0.00995155]\n",
      " [ 0.08283634  0.02301172 -0.54994371  0.97579028 -0.5060872   0.49827884\n",
      "   0.00629589  0.01965677 -0.5589169   0.00907797]\n",
      " [ 0.0621245   0.03254043 -0.86818864  1.0627003  -0.35109114  0.39285692\n",
      "   0.00493352  0.01667449 -0.35910594  0.00655556]\n",
      " [ 0.01692407  0.00907132 -0.24368973  0.29464401 -0.07020147  0.10552748\n",
      "   0.00145443  0.00416736 -0.11958723  0.00168976]\n",
      " [ 0.09390961  0.00571791 -0.00877228  0.6604368  -0.78984473  0.55181716\n",
      "   0.00609037  0.02256156 -0.55320376  0.01128736]]\n",
      "b grad\n",
      "[ 0.1430579   0.04116799 -0.99175638  1.71571976 -0.88482101  0.86325707\n",
      "  0.01083493  0.03434324 -0.94750734  0.01570385]\n",
      "vals grad\n",
      "[[-0.69598562 -2.36442889  0.48147152  3.29717836  3.21757735 -0.30464138\n",
      "  -1.83854208]\n",
      " [ 0.96673141  0.09039896  0.40032064 -0.52274885  1.79960559  1.06695152\n",
      "   0.65245033]\n",
      " [-0.82098398 -0.94328436 -0.21932967  0.69011007  0.63465749  3.28113597\n",
      "   1.38930906]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted)\n",
    "print(\"loss\")\n",
    "print(l)\n",
    "print(\"loss grad\")\n",
    "print(l_grad)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad)\n",
    "print(\"W grad\")\n",
    "print(dense.grads[0])\n",
    "print(\"b grad\")\n",
    "print(dense.grads[1])\n",
    "print(\"vals grad\")\n",
    "print(f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tf.Tensor(\n",
      "[[1.12266094e-02 3.40723142e-02 1.47358772e-03 8.03037682e-01\n",
      "  1.13033016e-02 1.04788833e-01 1.37781349e-03 6.22199035e-03\n",
      "  2.58615332e-02 6.36335544e-04]\n",
      " [6.35036520e-02 4.09659816e-03 4.64408156e-03 4.40106757e-01\n",
      "  3.58909784e-02 4.03573917e-01 2.27405181e-03 2.21779879e-02\n",
      "  1.45106324e-02 9.22134397e-03]\n",
      " [6.83276413e-02 2.99907419e-03 2.12594883e-03 4.72575321e-01\n",
      "  6.79847118e-02 3.54894315e-01 7.18306216e-03 5.94326080e-03\n",
      "  1.21204946e-02 5.84617006e-03]], shape=(3, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor([6.52005522 3.32726931 4.41285749], shape=(3,), dtype=float64)\n",
      "loss grad\n",
      "tf.Tensor(\n",
      "[[   1.            1.         -677.61586049    1.            1.\n",
      "     1.            1.            1.            1.            1.        ]\n",
      " [   1.            1.            1.            1.          -26.86215492\n",
      "     1.            1.            1.            1.            1.        ]\n",
      " [   1.            1.            1.            1.            1.\n",
      "     1.            1.            1.          -81.50488373    1.        ]], shape=(3, 10), dtype=float64)\n",
      "softmax grad\n",
      "tf.Tensor(\n",
      "[[ 1.12266094e-02  3.40723142e-02 -9.98526412e-01  8.03037682e-01\n",
      "   1.13033016e-02  1.04788833e-01  1.37781349e-03  6.22199035e-03\n",
      "   2.58615332e-02  6.36335544e-04]\n",
      " [ 6.35036520e-02  4.09659816e-03  4.64408156e-03  4.40106757e-01\n",
      "  -9.64109022e-01  4.03573917e-01  2.27405181e-03  2.21779879e-02\n",
      "   1.45106324e-02  9.22134397e-03]\n",
      " [ 6.83276413e-02  2.99907419e-03  2.12594883e-03  4.72575321e-01\n",
      "   6.79847118e-02  3.54894315e-01  7.18306216e-03  5.94326080e-03\n",
      "  -9.87879505e-01  5.84617006e-03]], shape=(3, 10), dtype=float64)\n",
      "W grad\n",
      "tf.Tensor(\n",
      "[[ 0.05638084  0.03019958 -0.80033036  0.97396841 -0.50256216  0.37033739\n",
      "   0.00369181  0.01818242 -0.15649144  0.0066235 ]\n",
      " [ 0.02387334  0.02321265 -0.65145359  0.63923074 -0.15216132  0.16699908\n",
      "   0.00188468  0.00835719 -0.06242252  0.00247975]\n",
      " [ 0.08376848  0.02364808 -0.55887248  0.98933851 -0.72128376  0.51907598\n",
      "   0.00546968  0.02329163 -0.37438767  0.00995155]\n",
      " [ 0.08283634  0.02301172 -0.54994371  0.97579028 -0.5060872   0.49827884\n",
      "   0.00629589  0.01965677 -0.5589169   0.00907797]\n",
      " [ 0.0621245   0.03254043 -0.86818864  1.0627003  -0.35109114  0.39285692\n",
      "   0.00493352  0.01667449 -0.35910594  0.00655556]\n",
      " [ 0.01692407  0.00907132 -0.24368973  0.29464401 -0.07020147  0.10552748\n",
      "   0.00145443  0.00416736 -0.11958723  0.00168976]\n",
      " [ 0.09390961  0.00571791 -0.00877228  0.6604368  -0.78984473  0.55181716\n",
      "   0.00609037  0.02256156 -0.55320376  0.01128736]], shape=(7, 10), dtype=float64)\n",
      "b grad\n",
      "tf.Tensor(\n",
      "[ 0.1430579   0.04116799 -0.99175638  1.71571976 -0.88482101  0.86325707\n",
      "  0.01083493  0.03434324 -0.94750734  0.01570385], shape=(10,), dtype=float64)\n",
      "vals grad\n",
      "tf.Tensor(\n",
      "[[-0.69598562 -2.36442889  0.48147152  3.29717836  3.21757735 -0.30464138\n",
      "  -1.83854208]\n",
      " [ 0.96673141  0.09039896  0.40032064 -0.52274885  1.79960559  1.06695152\n",
      "   0.65245033]\n",
      " [-0.82098398 -0.94328436 -0.21932967  0.69011007  0.63465749  3.28113597\n",
      "   1.38930906]], shape=(3, 7), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_tf)\n",
    "print(\"loss\")\n",
    "print(l_tf)\n",
    "print(\"loss grad\")\n",
    "print(l_grad_tf)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad_tf)\n",
    "print(\"W grad\")\n",
    "print(w_tf_grad)\n",
    "print(\"b grad\")\n",
    "print(b_tf_grad)\n",
    "print(\"vals grad\")\n",
    "print(f_tf_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tensor([[1.1227e-02, 3.4072e-02, 1.4736e-03, 8.0304e-01, 1.1303e-02, 1.0479e-01,\n",
      "         1.3778e-03, 6.2220e-03, 2.5862e-02, 6.3634e-04],\n",
      "        [6.3504e-02, 4.0966e-03, 4.6441e-03, 4.4011e-01, 3.5891e-02, 4.0357e-01,\n",
      "         2.2741e-03, 2.2178e-02, 1.4511e-02, 9.2213e-03],\n",
      "        [6.8328e-02, 2.9991e-03, 2.1259e-03, 4.7258e-01, 6.7985e-02, 3.5489e-01,\n",
      "         7.1831e-03, 5.9433e-03, 1.2120e-02, 5.8462e-03]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "loss\n",
      "tensor([6.5201, 3.3273, 4.4129], dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward>)\n",
      "loss grad\n",
      "tensor([[   0.0000,    0.0000, -678.6159,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,  -27.8622,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,  -82.5049,    0.0000]], dtype=torch.float64)\n",
      "softmax grad\n",
      "tensor([[ 1.1227e-02,  3.4072e-02, -9.9853e-01,  8.0304e-01,  1.1303e-02,\n",
      "          1.0479e-01,  1.3778e-03,  6.2220e-03,  2.5862e-02,  6.3634e-04],\n",
      "        [ 6.3504e-02,  4.0966e-03,  4.6441e-03,  4.4011e-01, -9.6411e-01,\n",
      "          4.0357e-01,  2.2741e-03,  2.2178e-02,  1.4511e-02,  9.2213e-03],\n",
      "        [ 6.8328e-02,  2.9991e-03,  2.1259e-03,  4.7258e-01,  6.7985e-02,\n",
      "          3.5489e-01,  7.1831e-03,  5.9433e-03, -9.8788e-01,  5.8462e-03]],\n",
      "       dtype=torch.float64)\n",
      "W grad\n",
      "tensor([[ 0.0564,  0.0302, -0.8003,  0.9740, -0.5026,  0.3703,  0.0037,  0.0182,\n",
      "         -0.1565,  0.0066],\n",
      "        [ 0.0239,  0.0232, -0.6515,  0.6392, -0.1522,  0.1670,  0.0019,  0.0084,\n",
      "         -0.0624,  0.0025],\n",
      "        [ 0.0838,  0.0236, -0.5589,  0.9893, -0.7213,  0.5191,  0.0055,  0.0233,\n",
      "         -0.3744,  0.0100],\n",
      "        [ 0.0828,  0.0230, -0.5499,  0.9758, -0.5061,  0.4983,  0.0063,  0.0197,\n",
      "         -0.5589,  0.0091],\n",
      "        [ 0.0621,  0.0325, -0.8682,  1.0627, -0.3511,  0.3929,  0.0049,  0.0167,\n",
      "         -0.3591,  0.0066],\n",
      "        [ 0.0169,  0.0091, -0.2437,  0.2946, -0.0702,  0.1055,  0.0015,  0.0042,\n",
      "         -0.1196,  0.0017],\n",
      "        [ 0.0939,  0.0057, -0.0088,  0.6604, -0.7898,  0.5518,  0.0061,  0.0226,\n",
      "         -0.5532,  0.0113]], dtype=torch.float64)\n",
      "b grad\n",
      "tensor([ 0.1431,  0.0412, -0.9918,  1.7157, -0.8848,  0.8633,  0.0108,  0.0343,\n",
      "        -0.9475,  0.0157], dtype=torch.float64)\n",
      "vals grad\n",
      "tensor([[-0.6960, -2.3644,  0.4815,  3.2972,  3.2176, -0.3046, -1.8385],\n",
      "        [ 0.9667,  0.0904,  0.4003, -0.5227,  1.7996,  1.0670,  0.6525],\n",
      "        [-0.8210, -0.9433, -0.2193,  0.6901,  0.6347,  3.2811,  1.3893]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_t)\n",
    "print(\"loss\")\n",
    "print(l_t)\n",
    "print(\"loss grad\")\n",
    "print(predicted_t.grad)\n",
    "print(\"softmax grad\")\n",
    "print(first_t.grad)\n",
    "print(\"W grad\")\n",
    "print(w_t.grad)\n",
    "print(\"b grad\")\n",
    "print(b_t.grad)\n",
    "print(\"vals grad\")\n",
    "print(vals_t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "loss\n",
      "[ True  True  True]\n",
      "loss grad\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "softmax grad\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "W grad\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "b grad\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "vals grad\n",
      "[[ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(np.abs(predicted - predicted_t.detach().numpy()) < 1e-12)\n",
    "print(\"loss\")\n",
    "print(np.abs(l - l_t.detach().numpy()) < 1e-12)\n",
    "print(\"loss grad\")\n",
    "print(np.abs(l_grad - predicted_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"softmax grad\")\n",
    "print(np.abs(s_grad - first_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"W grad\")\n",
    "print(np.abs(dense.grads[0] - w_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"b grad\")\n",
    "print(np.abs(dense.grads[1] - b_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"vals grad\")\n",
    "print(np.abs(f_grad - vals_t.grad.detach().numpy()) < 1e-12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

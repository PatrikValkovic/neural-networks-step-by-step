{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing, sklearn.datasets, sklearn.model_selection\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    def __call__(self, target, predicted):\n",
    "        indices = np.arange(len(target))\n",
    "        return -np.log(np.maximum(predicted[indices,target], 1e-15))\n",
    "    \n",
    "    def gradient(self, target, predicted):\n",
    "        grad = np.zeros((len(target), 10))\n",
    "        indices = np.arange(len(target))\n",
    "        grad[indices,target] = -1 / predicted[indices,target]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs - np.max(inputs)\n",
    "        return np.exp(inputs) / np.sum(np.exp(inputs), axis=-1)[:,np.newaxis]\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        outputs = self(inputs)  # examples, classes\n",
    "        examples, classes = outputs.shape\n",
    "        diag = np.zeros((examples, classes, classes))  # examples, classes, classes\n",
    "        diag[:, np.arange(classes), np.arange(classes)] = outputs # set the diagonal of each example\n",
    "        my_gradient = diag - outputs[:,:,np.newaxis] * outputs[:,np.newaxis,:]  # examples, classes, classes\n",
    "        return np.sum(gradients[:,np.newaxis,:] * my_gradient, axis=2) # examples, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self,inputs, outputs, random_seed=None):\n",
    "        self._random_state = np.random.RandomState(random_seed)\n",
    "        self._W = self._random_state.uniform(-2,2,size=(inputs, outputs))\n",
    "        self._b = self._random_state.uniform(-2,2,size=(outputs,))\n",
    "        self.params = [self._W, self._b]\n",
    "        self.grads = [np.zeros_like(self._W), np.zeros_like(self._b)]\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        return inputs @ self._W + self._b\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        # gradient in respect to W\n",
    "        w_grad = inputs[:,:,np.newaxis] * gradients[:,np.newaxis,:]  # examples, inputs, outputs\n",
    "        np.add(self.grads[0], np.sum(w_grad, axis=0), out=self.grads[0])  # inputs, outputs\n",
    "        # gradient in respect to b\n",
    "        b_grad = gradients  # examples, outputs\n",
    "        np.add(self.grads[1], np.sum(gradients, axis=0), out=self.grads[1])  # outputs\n",
    "        # gradient in respect to inputs\n",
    "        in_grad = self._W[np.newaxis,:,:] * gradients[:,np.newaxis,:] + np.sign(self._b)[np.newaxis, np.newaxis, :]  # examples, inputs, outputs\n",
    "        return np.sum(in_grad, axis=2) # examples, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.random.randint(0,10,size=(3,), dtype=int)\n",
    "vals = np.random.uniform(size=(3,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = DenseLayer(7,10,42)\n",
    "soft = SoftmaxLayer()\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "first = dense(vals)\n",
    "predicted = soft(first)\n",
    "l = loss(target, predicted)\n",
    "l_grad = loss.gradient(target, predicted)\n",
    "s_grad = soft.gradient(first, l_grad)\n",
    "f_grad = dense.gradient(vals, s_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w_tf = tf.Variable(dense._W)\n",
    "b_tf = tf.Variable(dense._b)\n",
    "target_tf = tf.Variable(target)\n",
    "vals_tf = tf.Variable(vals)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    first_tf = vals_tf @ w_tf + b_tf\n",
    "    predicted_tf = tf.nn.softmax(first_tf)\n",
    "    l_tf = tf.keras.losses.sparse_categorical_crossentropy(target_tf, predicted_tf)\n",
    "\n",
    "l_grad_tf, s_grad_tf, w_tf_grad, b_tf_grad, f_tf_grad = tape.gradient(l_tf, [predicted_tf, first_tf, w_tf, b_tf, vals_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w_t = torch.tensor(dense._W, requires_grad=True)\n",
    "b_t = torch.tensor(dense._b, requires_grad=True)\n",
    "target_t = torch.tensor(target, dtype=torch.long)\n",
    "vals_t = torch.tensor(vals, requires_grad=True)\n",
    "\n",
    "first_t = vals_t @ w_t + b_t\n",
    "first_t.retain_grad()\n",
    "predicted_t = torch.nn.functional.softmax(first_t, dim=1)\n",
    "predicted_t.retain_grad()\n",
    "l_t = torch.nn.functional.nll_loss(torch.log(predicted_t), target_t, reduction='none')\n",
    "l_t.backward(torch.ones(l_t.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "[[5.23538903e-02 5.42054255e-02 9.70625661e-03 4.35072983e-01\n",
      "  4.50116097e-02 3.51086763e-01 1.10968445e-02 8.94194225e-03\n",
      "  3.14159738e-02 1.10831134e-03]\n",
      " [5.71876434e-02 1.85377267e-03 7.47330390e-04 5.50580004e-01\n",
      "  3.33689323e-02 3.42114524e-01 1.31423331e-03 2.71495398e-03\n",
      "  9.01596837e-03 1.10263764e-03]\n",
      " [3.06704943e-02 1.45364088e-03 1.25016358e-03 7.06644851e-01\n",
      "  1.50051632e-02 2.40874780e-01 1.27466093e-04 1.15820776e-03\n",
      "  1.12816896e-03 1.68706444e-03]]\n",
      "loss\n",
      "[4.63498459 6.29053243 1.42347807]\n",
      "loss grad\n",
      "[[   0.            0.         -103.02633038    0.            0.\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.         -539.44046884    0.            0.            0.\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "    -4.15153467    0.            0.            0.            0.        ]]\n",
      "softmax grad\n",
      "[[ 5.23538903e-02  5.42054255e-02 -9.90293743e-01  4.35072983e-01\n",
      "   4.50116097e-02  3.51086763e-01  1.10968445e-02  8.94194225e-03\n",
      "   3.14159738e-02  1.10831134e-03]\n",
      " [ 5.71876434e-02 -9.98146227e-01  7.47330390e-04  5.50580004e-01\n",
      "   3.33689323e-02  3.42114524e-01  1.31423331e-03  2.71495398e-03\n",
      "   9.01596837e-03  1.10263764e-03]\n",
      " [ 3.06704943e-02  1.45364088e-03  1.25016358e-03  7.06644851e-01\n",
      "   1.50051632e-02 -7.59125220e-01  1.27466093e-04  1.15820776e-03\n",
      "   1.12816896e-03  1.68706444e-03]]\n",
      "W grad\n",
      "[[ 5.59557013e-02 -4.58930636e-01 -1.80097235e-01  7.87084114e-01\n",
      "   3.34629091e-02 -2.56355285e-01  2.73033764e-03  3.64914609e-03\n",
      "   1.07087997e-02  1.79214819e-03]\n",
      " [ 4.88470847e-02  3.40992439e-02 -8.34783992e-01  4.66175878e-01\n",
      "   4.03168738e-02  2.00320868e-01  9.38847920e-03  7.72356817e-03\n",
      "   2.67428505e-02  1.16914593e-03]\n",
      " [ 9.32275081e-02 -7.24062028e-01 -6.67038152e-01  1.04259787e+00\n",
      "   6.28106310e-02  1.44297021e-01  8.54924478e-03  8.64364592e-03\n",
      "   2.85997317e-02  2.37452312e-03]\n",
      " [ 8.25697555e-02 -6.52837617e-01 -2.66559702e-01  1.17943343e+00\n",
      "   4.92641981e-02 -4.19611155e-01  4.01201943e-03  5.37718686e-03\n",
      "   1.56591828e-02  2.69270247e-03]\n",
      " [ 7.20575234e-02 -4.92557524e-01 -4.79873296e-01  8.86330419e-01\n",
      "   4.74808032e-02 -6.85857640e-02  6.14363941e-03  6.39286111e-03\n",
      "   2.05717731e-02  2.03956553e-03]\n",
      " [ 5.69977111e-02 -3.02528582e-01 -3.29317309e-01  8.08777761e-01\n",
      "   3.60946534e-02 -2.94942495e-01  4.21370344e-03  4.65523962e-03\n",
      "   1.41634488e-02  1.88586869e-03]\n",
      " [ 4.96907726e-02 -3.76768014e-01 -2.28729095e-02  8.34475245e-01\n",
      "   2.68342666e-02 -5.21589820e-01  8.82020820e-04  2.25806312e-03\n",
      "   5.17693170e-03  1.91344442e-03]]\n",
      "b grad\n",
      "[ 0.14021203 -0.94248716 -0.98829625  1.69229784  0.09338571 -0.06592393\n",
      "  0.01253854  0.0128151   0.04156011  0.00389801]\n",
      "vals grad\n",
      "[[-1.2137476  -2.33516531  0.92539769  2.97193803  2.65814841 -0.3728025\n",
      "  -1.66028352]\n",
      " [-2.12069555 -3.09820077  1.55429843  2.81251404  1.0225909   0.44752497\n",
      "   0.59033709]\n",
      " [ 1.2945137   0.07302711 -1.2384567   0.36787634  0.59835495 -0.10257671\n",
      "  -0.55845347]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted)\n",
    "print(\"loss\")\n",
    "print(l)\n",
    "print(\"loss grad\")\n",
    "print(l_grad)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad)\n",
    "print(\"W grad\")\n",
    "print(dense.grads[0])\n",
    "print(\"b grad\")\n",
    "print(dense.grads[1])\n",
    "print(\"vals grad\")\n",
    "print(f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tf.Tensor(\n",
      "[[5.23538903e-02 5.42054255e-02 9.70625661e-03 4.35072983e-01\n",
      "  4.50116097e-02 3.51086763e-01 1.10968445e-02 8.94194225e-03\n",
      "  3.14159738e-02 1.10831134e-03]\n",
      " [5.71876434e-02 1.85377267e-03 7.47330390e-04 5.50580004e-01\n",
      "  3.33689323e-02 3.42114524e-01 1.31423331e-03 2.71495398e-03\n",
      "  9.01596837e-03 1.10263764e-03]\n",
      " [3.06704943e-02 1.45364088e-03 1.25016358e-03 7.06644851e-01\n",
      "  1.50051632e-02 2.40874780e-01 1.27466093e-04 1.15820776e-03\n",
      "  1.12816896e-03 1.68706444e-03]], shape=(3, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor([4.63498459 6.29053243 1.42347807], shape=(3,), dtype=float64)\n",
      "loss grad\n",
      "tf.Tensor(\n",
      "[[   1.            1.         -102.02633038    1.            1.\n",
      "     1.            1.            1.            1.            1.        ]\n",
      " [   1.         -538.44046884    1.            1.            1.\n",
      "     1.            1.            1.            1.            1.        ]\n",
      " [   1.            1.            1.            1.            1.\n",
      "    -3.15153467    1.            1.            1.            1.        ]], shape=(3, 10), dtype=float64)\n",
      "softmax grad\n",
      "tf.Tensor(\n",
      "[[ 5.23538903e-02  5.42054255e-02 -9.90293743e-01  4.35072983e-01\n",
      "   4.50116097e-02  3.51086763e-01  1.10968445e-02  8.94194225e-03\n",
      "   3.14159738e-02  1.10831134e-03]\n",
      " [ 5.71876434e-02 -9.98146227e-01  7.47330390e-04  5.50580004e-01\n",
      "   3.33689323e-02  3.42114524e-01  1.31423331e-03  2.71495398e-03\n",
      "   9.01596837e-03  1.10263764e-03]\n",
      " [ 3.06704943e-02  1.45364088e-03  1.25016358e-03  7.06644851e-01\n",
      "   1.50051632e-02 -7.59125220e-01  1.27466093e-04  1.15820776e-03\n",
      "   1.12816896e-03  1.68706444e-03]], shape=(3, 10), dtype=float64)\n",
      "W grad\n",
      "tf.Tensor(\n",
      "[[ 5.59557013e-02 -4.58930636e-01 -1.80097235e-01  7.87084114e-01\n",
      "   3.34629091e-02 -2.56355285e-01  2.73033764e-03  3.64914609e-03\n",
      "   1.07087997e-02  1.79214819e-03]\n",
      " [ 4.88470847e-02  3.40992439e-02 -8.34783992e-01  4.66175878e-01\n",
      "   4.03168738e-02  2.00320868e-01  9.38847920e-03  7.72356817e-03\n",
      "   2.67428505e-02  1.16914593e-03]\n",
      " [ 9.32275081e-02 -7.24062028e-01 -6.67038152e-01  1.04259787e+00\n",
      "   6.28106310e-02  1.44297021e-01  8.54924478e-03  8.64364592e-03\n",
      "   2.85997317e-02  2.37452312e-03]\n",
      " [ 8.25697555e-02 -6.52837617e-01 -2.66559702e-01  1.17943343e+00\n",
      "   4.92641981e-02 -4.19611155e-01  4.01201943e-03  5.37718686e-03\n",
      "   1.56591828e-02  2.69270247e-03]\n",
      " [ 7.20575234e-02 -4.92557524e-01 -4.79873296e-01  8.86330419e-01\n",
      "   4.74808032e-02 -6.85857640e-02  6.14363941e-03  6.39286111e-03\n",
      "   2.05717731e-02  2.03956553e-03]\n",
      " [ 5.69977111e-02 -3.02528582e-01 -3.29317309e-01  8.08777761e-01\n",
      "   3.60946534e-02 -2.94942495e-01  4.21370344e-03  4.65523962e-03\n",
      "   1.41634488e-02  1.88586869e-03]\n",
      " [ 4.96907726e-02 -3.76768014e-01 -2.28729095e-02  8.34475245e-01\n",
      "   2.68342666e-02 -5.21589820e-01  8.82020820e-04  2.25806312e-03\n",
      "   5.17693170e-03  1.91344442e-03]], shape=(7, 10), dtype=float64)\n",
      "b grad\n",
      "tf.Tensor(\n",
      "[ 0.14021203 -0.94248716 -0.98829625  1.69229784  0.09338571 -0.06592393\n",
      "  0.01253854  0.0128151   0.04156011  0.00389801], shape=(10,), dtype=float64)\n",
      "vals grad\n",
      "tf.Tensor(\n",
      "[[-1.2137476  -2.33516531  0.92539769  2.97193803  2.65814841 -0.3728025\n",
      "  -1.66028352]\n",
      " [-2.12069555 -3.09820077  1.55429843  2.81251404  1.0225909   0.44752497\n",
      "   0.59033709]\n",
      " [ 1.2945137   0.07302711 -1.2384567   0.36787634  0.59835495 -0.10257671\n",
      "  -0.55845347]], shape=(3, 7), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_tf)\n",
    "print(\"loss\")\n",
    "print(l_tf)\n",
    "print(\"loss grad\")\n",
    "print(l_grad_tf)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad_tf)\n",
    "print(\"W grad\")\n",
    "print(w_tf_grad)\n",
    "print(\"b grad\")\n",
    "print(b_tf_grad)\n",
    "print(\"vals grad\")\n",
    "print(f_tf_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "tensor([[5.2354e-02, 5.4205e-02, 9.7063e-03, 4.3507e-01, 4.5012e-02, 3.5109e-01,\n",
      "         1.1097e-02, 8.9419e-03, 3.1416e-02, 1.1083e-03],\n",
      "        [5.7188e-02, 1.8538e-03, 7.4733e-04, 5.5058e-01, 3.3369e-02, 3.4211e-01,\n",
      "         1.3142e-03, 2.7150e-03, 9.0160e-03, 1.1026e-03],\n",
      "        [3.0670e-02, 1.4536e-03, 1.2502e-03, 7.0664e-01, 1.5005e-02, 2.4087e-01,\n",
      "         1.2747e-04, 1.1582e-03, 1.1282e-03, 1.6871e-03]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "loss\n",
      "tensor([4.6350, 6.2905, 1.4235], dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward>)\n",
      "loss grad\n",
      "tensor([[   0.0000,    0.0000, -103.0263,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000, -539.4405,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,   -4.1515,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000]], dtype=torch.float64)\n",
      "softmax grad\n",
      "tensor([[ 5.2354e-02,  5.4205e-02, -9.9029e-01,  4.3507e-01,  4.5012e-02,\n",
      "          3.5109e-01,  1.1097e-02,  8.9419e-03,  3.1416e-02,  1.1083e-03],\n",
      "        [ 5.7188e-02, -9.9815e-01,  7.4733e-04,  5.5058e-01,  3.3369e-02,\n",
      "          3.4211e-01,  1.3142e-03,  2.7150e-03,  9.0160e-03,  1.1026e-03],\n",
      "        [ 3.0670e-02,  1.4536e-03,  1.2502e-03,  7.0664e-01,  1.5005e-02,\n",
      "         -7.5913e-01,  1.2747e-04,  1.1582e-03,  1.1282e-03,  1.6871e-03]],\n",
      "       dtype=torch.float64)\n",
      "W grad\n",
      "tensor([[ 5.5956e-02, -4.5893e-01, -1.8010e-01,  7.8708e-01,  3.3463e-02,\n",
      "         -2.5636e-01,  2.7303e-03,  3.6491e-03,  1.0709e-02,  1.7921e-03],\n",
      "        [ 4.8847e-02,  3.4099e-02, -8.3478e-01,  4.6618e-01,  4.0317e-02,\n",
      "          2.0032e-01,  9.3885e-03,  7.7236e-03,  2.6743e-02,  1.1691e-03],\n",
      "        [ 9.3228e-02, -7.2406e-01, -6.6704e-01,  1.0426e+00,  6.2811e-02,\n",
      "          1.4430e-01,  8.5492e-03,  8.6436e-03,  2.8600e-02,  2.3745e-03],\n",
      "        [ 8.2570e-02, -6.5284e-01, -2.6656e-01,  1.1794e+00,  4.9264e-02,\n",
      "         -4.1961e-01,  4.0120e-03,  5.3772e-03,  1.5659e-02,  2.6927e-03],\n",
      "        [ 7.2058e-02, -4.9256e-01, -4.7987e-01,  8.8633e-01,  4.7481e-02,\n",
      "         -6.8586e-02,  6.1436e-03,  6.3929e-03,  2.0572e-02,  2.0396e-03],\n",
      "        [ 5.6998e-02, -3.0253e-01, -3.2932e-01,  8.0878e-01,  3.6095e-02,\n",
      "         -2.9494e-01,  4.2137e-03,  4.6552e-03,  1.4163e-02,  1.8859e-03],\n",
      "        [ 4.9691e-02, -3.7677e-01, -2.2873e-02,  8.3448e-01,  2.6834e-02,\n",
      "         -5.2159e-01,  8.8202e-04,  2.2581e-03,  5.1769e-03,  1.9134e-03]],\n",
      "       dtype=torch.float64)\n",
      "b grad\n",
      "tensor([ 0.1402, -0.9425, -0.9883,  1.6923,  0.0934, -0.0659,  0.0125,  0.0128,\n",
      "         0.0416,  0.0039], dtype=torch.float64)\n",
      "vals grad\n",
      "tensor([[-1.2137, -2.3352,  0.9254,  2.9719,  2.6581, -0.3728, -1.6603],\n",
      "        [-2.1207, -3.0982,  1.5543,  2.8125,  1.0226,  0.4475,  0.5903],\n",
      "        [ 1.2945,  0.0730, -1.2385,  0.3679,  0.5984, -0.1026, -0.5585]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_t)\n",
    "print(\"loss\")\n",
    "print(l_t)\n",
    "print(\"loss grad\")\n",
    "print(predicted_t.grad)\n",
    "print(\"softmax grad\")\n",
    "print(first_t.grad)\n",
    "print(\"W grad\")\n",
    "print(w_t.grad)\n",
    "print(\"b grad\")\n",
    "print(b_t.grad)\n",
    "print(\"vals grad\")\n",
    "print(vals_t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "loss\n",
      "[ True  True  True]\n",
      "loss grad\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "softmax grad\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "W grad\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]]\n",
      "b grad\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "vals grad\n",
      "[[ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\")\n",
    "print(np.abs(predicted - predicted_t.detach().numpy()) < 1e-12)\n",
    "print(\"loss\")\n",
    "print(np.abs(l - l_t.detach().numpy()) < 1e-12)\n",
    "print(\"loss grad\")\n",
    "print(np.abs(l_grad - predicted_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"softmax grad\")\n",
    "print(np.abs(s_grad - first_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"W grad\")\n",
    "print(np.abs(dense.grads[0] - w_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"b grad\")\n",
    "print(np.abs(dense.grads[1] - b_t.grad.detach().numpy()) < 1e-12)\n",
    "print(\"vals grad\")\n",
    "print(np.abs(f_grad - vals_t.grad.detach().numpy()) < 1e-12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

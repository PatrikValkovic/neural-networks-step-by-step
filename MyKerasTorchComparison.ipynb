{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing, sklearn.datasets, sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    def __call__(self, target, predicted):\n",
    "        indices = np.arange(len(target))\n",
    "        return -np.log(np.maximum(predicted[indices,target], 1e-15))\n",
    "    \n",
    "    def gradient(self, target, predicted):\n",
    "        grad = np.zeros((len(target), 10))\n",
    "        indices = np.arange(len(target))\n",
    "        grad[indices,target] = -1 / predicted[indices,target]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        inputs = inputs - np.max(inputs)\n",
    "        return np.exp(inputs) / np.sum(np.exp(inputs), axis=-1)[:,np.newaxis]\n",
    "    \n",
    "    def gradient(self, inputs, gradients):\n",
    "        outputs = self(inputs)  # examples, classes\n",
    "        examples, classes = outputs.shape\n",
    "        diag = np.zeros((examples, classes, classes))  # examples, classes, classes\n",
    "        diag[:, np.arange(classes), np.arange(classes)] = outputs # set the diagonal of each example\n",
    "        my_gradient = diag - outputs[:,:,np.newaxis] * outputs[:,np.newaxis,:]  # examples, classes, classes\n",
    "        return np.sum(gradients[:,np.newaxis,:] * my_gradient, axis=2) # examples, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseLayer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.random.randint(0,10,size=(3,), dtype=int)\n",
    "vals = np.random.uniform(size=(3,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft = SoftmaxLayer()\n",
    "loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "predicted = soft(vals)\n",
    "l = loss(target, predicted)\n",
    "l_grad = loss.gradient(target, predicted)\n",
    "s_grad = soft.gradient(vals, l_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "target_tf = tf.Variable(target)\n",
    "vals_tf = tf.Variable(vals)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predicted_tf = tf.nn.softmax(vals_tf)\n",
    "    l_tf = tf.keras.losses.sparse_categorical_crossentropy(target_tf, predicted_tf)\n",
    "\n",
    "l_grad_tf, s_grad_tf = tape.gradient(l_tf, [predicted_tf, vals_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "target_t = torch.tensor(target, dtype=torch.long)\n",
    "vals_t = torch.tensor(vals, requires_grad=True)\n",
    "\n",
    "predicted_t = torch.nn.functional.softmax(vals_t, dim=1)\n",
    "predicted_t.retain_grad()\n",
    "l_t = torch.nn.functional.nll_loss(torch.log(predicted_t), target_t, reduction='none')\n",
    "l_t.backward(torch.ones(l_t.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted)\n",
    "print(\"loss\")\n",
    "print(l)\n",
    "print(\"loss grad\")\n",
    "print(l_grad)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_tf)\n",
    "print(\"loss\")\n",
    "print(l_tf)\n",
    "print(\"loss grad\")\n",
    "print(l_grad_tf)\n",
    "print(\"softmax grad\")\n",
    "print(s_grad_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction\")\n",
    "print(predicted_t)\n",
    "print(\"loss\")\n",
    "print(l_t)\n",
    "print(\"loss grad\")\n",
    "print(predicted_t.grad)\n",
    "print(\"softmax grad\")\n",
    "print(vals_t.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

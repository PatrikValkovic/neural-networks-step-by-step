{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the series of jupyter notebooks about neural networks. The goal of this repository is to teach you neural networks from the beginning up to the state of the art techniques. Unlike other resources, my goal is to put everything as plainly as possible but still present the ideas behind. To help with that, I wanted to show as much as possible on real data and using graphs and other techniques. As so, this repository doesn't contain only description of neural networks, but many real examples as well, that you may try yourself and experience with.\n",
    "\n",
    "The notebooks are usually structured into three or four parts. Firstly, I show and describe the problem we are facing. Follows discussion of how to solve this problem and the pros and cons of the approaches. I then took the best solutions and show the code and/or the mathematical background. I believe this approach helps to better understand both the technique, as well why we use it and when to use it.\n",
    "\n",
    "# Requirements\n",
    "\n",
    "Although I wanted to make these materials for as broad audience as possible, I can't explain everything from elementary school. At least the following knowledge is required to fully utilize this repository.\n",
    "\n",
    "- Python programming with an object-oriented paradigm. All the codes in this repository are in Python and I may use advanced techniques like classes, generators, or annotations to simplify the code. There are plenty of Python programming tutorials online if you not confident.\n",
    "- You should know basic mathematical Python libraries like NumPy, Matplotlib, Pandas, and more. Through the notebooks, I will show you Pytorch and TensorFlow libraries and no prior knowledge of these is required. Nevertheless, these libraries have fundamentals and API in NumPy and I will use NumPy through the first notebooks to show the basics. You may start with the [cs231n/python-numpy-tutorial](https://cs231n.github.io/python-numpy-tutorial/) tutorial.\n",
    "- Mathematical analysis is another field, that we will build the neural networks on (you will need mainly derivatives and partial derivates). I will explain the ideas behind some of the formulas, but for the rest I will assume you know why the formula changed.\n",
    "- At least fundamental knowledge about probability theory and statistics is required as well. I will work with probabilities a lot and as so you should know the theory behind. \n",
    "\n",
    "# Naming conventions\n",
    "\n",
    "I will denote $P(X=x)$ as probability, that variable $X$ has value $x$, especially $P(f(x)=t)$ denotes probablity, that prediction (denoted by $f$ function) predicts correct (target) value $t$ for record $x$. Most of the time, we are interested in prediction over the whole dataset (set of data points), so I will denote $x_i$ as *i*-th datapoint and $t_i$ it's target (the value that should the estimator predict). Put it together, I will use $P(f(x_i)=t_i)$ most of the time.\n",
    "\n",
    "From the mathematical analysis point of view, I will denote $f(x)$ function of one variable $x$, $f(x_1,x_2,\\dots,x_n)$ function of $n$ variables named $x_1, x_2, \\dots, x_n$ or simply $f(\\pmb{x})$ where $\\pmb{x}$ is vector of parameters $\\pmb{x}=(x_1,x_2,\\dots,x_n)$.\n",
    "\n",
    "Moreover, I will denote $f'(x)$ or $f'$ as derivative of single parameter function $f$ by its only variable $x$. For functions with multiple parameters I will denote $\\frac{\\partial f(\\pmb{x})}{\\partial p}$ or $\\frac{\\partial f}{\\partial p}$ as partial derivation of function $f$ by parameter $p$.\n",
    "\n",
    "Lastly, I will denote $\\nabla f$ as gradient of function $f$, i.e. vector of derivatives for function $f$ by each parameter. Formally for function $f(x_1, x_2, \\dots, x_k)$ is \n",
    "$$\n",
    "\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\cdots, \\frac{\\partial f}{\\partial x_k} \\right)\n",
    "$$\n",
    "At some point, we will need higher-order derivatives, so I will denote $\\nabla^2 f$ as a second-order derivative of a function $f$ i.e. Hessian or generally $\\nabla^k f$ as *k*-order derivative of function $f$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
